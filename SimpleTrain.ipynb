{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Train\n",
    "\n",
    "This Notebook is written to provide a self-contained overview over the data and network loading and training\n",
    "\n",
    "It is organized in three parts\n",
    "\n",
    "1. Creation of a fake dataset with random values\n",
    "2. Loading this dataset using the Tensorflows [Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "3. Performing one training step\n",
    "\n",
    "## load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from S2parser_africa import S2parser\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "parser=S2parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filepath = '/home/roserustowicz/croptype_data_local/data/ghana/data.hdf5'\n",
    "grid_dir = '/home/roserustowicz/croptype_data_local/data/ghana'\n",
    "\n",
    "use_s2 = True\n",
    "country = 'ghana'\n",
    "dataset = 'full'\n",
    "\n",
    "S2_BAND_MEANS = { 'ghana': np.array([2620.00, 2519.89, 2630.31, 2739.81, 3225.22, 3562.64, 3356.57, 3788.05, 2915.40, 2102.65]),\n",
    "                  'southsudan': np.array([2119.15, 2061.95, 2127.71, 2277.60, 2784.21, 3088.40, 2939.33, 3308.03, 2597.14, 1834.81]),\n",
    "                  'tanzania': np.array([2551.54, 2471.35, 2675.69, 2799.99, 3191.33, 3453.16, 3335.64, 3660.05, 3182.23, 2383.79])}\n",
    "\n",
    "S2_BAND_STDS = { 'ghana': np.array([2171.62, 2085.69, 2174.37, 2084.56, 2058.97, 2117.31, 1988.70, 2099.78, 1209.48, 918.19]),\n",
    "                 'southsudan': np.array([2113.41, 2026.64, 2126.10, 2093.35, 2066.81, 2114.85, 2049.70, 2111.51, 1320.97, 1029.58]),\n",
    "                 'tanzania': np.array([2290.97, 2204.75, 2282.90, 2214.60, 2182.51, 2226.10, 2116.62, 2210.47, 1428.33, 1135.21])}\n",
    "\n",
    "MIN_TIMESTAMPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_field_sample(hdf5_filepath, filepath, country): \n",
    "    grid_id = filepath.split('/')[-1].replace(\".tfrecord\", \"\")\n",
    "    with h5py.File(hdf5_filepath, 'r') as data:\n",
    "        s2 = None\n",
    "        s2_doy = None\n",
    "\n",
    "        s2 = data['s2'][grid_id]\n",
    "        #s2 = normalization(s2, country)\n",
    "        s2_doy = data['s2_dates'][grid_id][()]\n",
    "        s2, s2_doy, _ = sample_timeseries(s2, MIN_TIMESTAMPS, dates=s2_doy, sample_w_clouds=False)\n",
    "\n",
    "        year = (np.ones((len(s2_doy), )) * 2017).astype(int)\n",
    "        \n",
    "        grid = moveTimeToStart(s2)\n",
    "        grid = np.transpose(grid, (0, 2, 3, 1))\n",
    "        label = data['labels'][grid_id][()]\n",
    "        \n",
    "    parser = S2parser()\n",
    "    parser.write(filepath, s2, s2_doy, year, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_random_sample(filepath,nobs=46, pix10=24, bands10=4, pix20=12, bands20=6, pix60=6, bands60=3):\n",
    "    \"\"\"create a fake sample with random values\"\"\"\n",
    "    x10 = (np.random.random([nobs,pix10,pix10,bands10])*1e4).astype(np.int64)\n",
    "    x20 = (np.random.random([nobs,pix20,pix20,bands20])*1e4).astype(np.int64)\n",
    "    x60 = (np.random.random([nobs,pix60,pix60,bands60])*1e4).astype(np.int64)\n",
    "    doy = (np.random.random([nobs])*365).astype(np.int64)\n",
    "    year = np.round(np.random.random([nobs])+2016.5).astype(np.int64)\n",
    "    label = (np.random.random([nobs,pix10,pix10])*17).astype(np.int64)\n",
    "\n",
    "    # create instance of parser\n",
    "    parser = S2parser()\n",
    "\n",
    "    # write .tfrecord\n",
    "    parser.write(filepath, x10, x20, x60, doy, year, label)\n",
    "\n",
    "def ziptfrecord(infile,outfile):\n",
    "    # gzip .tfrecord to .tfrecord.gz\n",
    "    with open(infile, 'rb') as f_in:\n",
    "        with gzip.open(outfile, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(grid, country):\n",
    "    \"\"\" Normalization based on values defined in constants.py\n",
    "    Args: \n",
    "      grid - (tensor) grid to be normalized\n",
    "      satellite - (str) describes source that grid is from (\"s1\" or \"s2\")\n",
    "\n",
    "    Returns:\n",
    "      grid - (tensor) a normalized version of the input grid\n",
    "    \"\"\"\n",
    "    num_bands = grid.shape[0]\n",
    "    s2_band_means = S2_BAND_MEANS[country]\n",
    "    s2_band_stds = S2_BAND_STDS[country]\n",
    "    grid = (grid-s2_band_means[:num_bands].reshape(num_bands, 1, 1, 1))/s2_band_stds[:num_bands].reshape(num_bands, 1, 1, 1)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveTimeToStart(arr):\n",
    "    return np.transpose(arr, [3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_timeseries(img_stack, num_samples, dates=None, timestamps_first=False, sample_w_clouds=True):\n",
    "    timestamps = img_stack.shape[3]\n",
    "    scores = np.ones((timestamps,))\n",
    "\n",
    "    # Compute probabilities of scores with softmax\n",
    "    probabilities = softmax(scores)\n",
    "    samples = np.random.choice(timestamps, size=num_samples, replace=False, p=probabilities)\n",
    "\n",
    "    # Sort samples to maintain sequential ordering\n",
    "    samples.sort()\n",
    "\n",
    "    # Use sampled indices to sample image and cloud stacks\n",
    "    sampled_img_stack = img_stack[:, :, :, samples]\n",
    "\n",
    "    # Samples dates\n",
    "    sampled_dates = None\n",
    "    if dates is not None:\n",
    "        sampled_dates = dates[samples]\n",
    "\n",
    "    return sampled_img_stack, sampled_dates, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes softmax values for a vector x.\n",
    "\n",
    "    Args: \n",
    "      x - (numpy array) a vector of real values\n",
    "\n",
    "    Returns: a vector of probabilities, of the same dimensions as x\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory to store the fake dataset\n",
    "directory=\"/home/roserustowicz/MTLCC-tf-fork/tmp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "for split in ['train', 'val', 'test']:\n",
    "    grid_path = os.path.join(grid_dir, f\"{country}_{dataset}_{split}\")\n",
    "    with open(grid_path, \"rb\") as f:\n",
    "        grid_list = list(pickle.load(f))\n",
    "            \n",
    "    filepaths=[\"{}/{}.tfrecord\".format(directory,i) for i in grid_list]\n",
    "    zippedfilepaths=[\"{}/{}.tfrecord.gz\".format(directory,i) for i in grid_list]\n",
    "\n",
    "# create the dataset\n",
    "#for filepath, zippedfilepath in zip(filepaths, zippedfilepaths):\n",
    "#    write_field_sample(hdf5_filepath, filepath, country)\n",
    "#    print(\"writing \"+filepath)\n",
    "#    ziptfrecord(filepath,zippedfilepath)\n",
    "#    print(\"compressing \"+filepath+\" -> \"+zippedfilepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Tensorflow Dataset Iterator\n",
    "\n",
    "See https://www.tensorflow.org/programmers_guide/datasets for more details\n",
    "\n",
    "- Essentially includes the data in the processing graph as input nodes.\n",
    "- These commands set up the processing graph of the input pipeline.\n",
    "- No data is processed,yet.\n",
    "- Some errors may remain undetected until data is inferred with sess.run(.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset all previously loaded graphs (in case this or the next cells have been executed twice)\n",
      "creating dataset object\n",
      "applying the mapping function on all samples (will read tfrecord file and normalize the values)\n",
      "repeat forever until externally stopped\n",
      "combine samples to batches\n",
      "WARNING:tensorflow:From <ipython-input-10-b49688913c0f>:37: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
      "make iterator\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "batchsize=2\n",
    "\n",
    "print(\"Reset all previously loaded graphs (in case this or the next cells have been executed twice)\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(\"creating dataset object\")\n",
    "dataset = tf.data.TFRecordDataset(zippedfilepaths, compression_type=\"GZIP\")\n",
    "#dataset = tf.data.TFRecordDataset(filepaths)\n",
    "\n",
    "def normalize(serialized_feature):\n",
    "    \"\"\" normalize stored integer values to floats approx. [0,1] \"\"\"\n",
    "    x10, doy, year, labels = serialized_feature\n",
    "    #x10, x20, x60, doy, year, labels = serialized_feature\n",
    "    x10 = tf.scalar_mul(1e-4, tf.cast(x10, tf.float32))\n",
    "    #x20 = tf.scalar_mul(1e-4, tf.cast(x20, tf.float32))\n",
    "    #x60 = tf.scalar_mul(1e-4, tf.cast(x60, tf.float32))\n",
    "    doy = tf.cast(doy, tf.float32) / 365\n",
    "    year = tf.cast(year, tf.float32) - 2016\n",
    "\n",
    "    #return x10, x20, x60, doy, year, labels\n",
    "    return x10, doy, year, labels\n",
    "\n",
    "def mapping_function(serialized_feature):\n",
    "    # read data from .tfrecords\n",
    "    serialized_feature = parser.parse_example(serialized_feature)\n",
    "    return normalize(serialized_feature)\n",
    "\n",
    "print(\"applying the mapping function on all samples (will read tfrecord file and normalize the values)\")\n",
    "dataset = dataset.map(mapping_function)\n",
    "\n",
    "print(\"repeat forever until externally stopped\")\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "print(\"combine samples to batches\")\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(int(batchsize)))\n",
    "\n",
    "print(\"make iterator\")\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Retrieve one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-65684ea3b610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrieving one sample as numpy array (just for fun)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/croptype/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/croptype/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    print(\"retrieving one sample as numpy array (just for fun)\")\n",
    "    x10, doy, year, labels = sess.run(iterator.get_next())\n",
    "    print(\"x10.shape: \" + str(x10.shape))\n",
    "    plt.imshow(x10[0,0,:,:,0])\n",
    "    plt.title(\"x10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b Perform one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d75ea680f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tmp/convgru128/graph.meta\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading network graph definition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/croptype/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/croptype/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "# define the network to be loaded\n",
    "# if not yet created, make one with python modelzoo/seqencoder.py script, as described in the readme.md\n",
    "graph=\"tmp/convgru128/graph.meta\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    print(\"loading network graph definition\")\n",
    "    tf.train.import_meta_graph(graph)\n",
    "\n",
    "    print(\"initializing variables, tables and the data iterator\")\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer(),iterator.initializer])\n",
    "\n",
    "    print(\"getting one string handle from the iterator that can be fed to the network\")\n",
    "    iterator_handle = sess.run(iterator.string_handle())\n",
    "\n",
    "    print(\"making some processing nodes accessible to python\")\n",
    "    def get_operation(name):\n",
    "        return tf.get_default_graph().get_operation_by_name(name).outputs[0]\n",
    "\n",
    "    iterator_handle_op = get_operation(\"data_iterator_handle\")\n",
    "    is_train_op = get_operation(\"is_train\")\n",
    "    train_op = get_operation(\"train_op\")\n",
    "\n",
    "    print(\"performing one training step\")\n",
    "    sess.run(train_op,feed_dict={iterator_handle_op: iterator_handle, is_train_op: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:croptype]",
   "language": "python",
   "name": "conda-env-croptype-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
